{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3 align = center>Hugging face transformers for sentiment analysis.</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers pipeline makes it easy to access the models like classifier, tokenizer or any other available model in Hugging face to performe the diffrent tasks such as NLP, Computer vision etc.. [Tasks](https://huggingface.co/tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pavankumar/Documents/Winter_Semester24/LLm/LLm_course/LLmvenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline for `named entity recognition` Here let's choose a model available in hugging face. [Reference](https://huggingface.co/learn/nlp-course/en/chapter1/3?fw=pt), [Choosen model](https://huggingface.co/models?pipeline_tag=token-classification&sort=trending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline('ner', model = \"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.99910825,\n",
       "  'index': 3,\n",
       "  'word': 'Pa',\n",
       "  'start': 6,\n",
       "  'end': 8},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.97229,\n",
       "  'index': 4,\n",
       "  'word': '##van',\n",
       "  'start': 8,\n",
       "  'end': 11},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9955035,\n",
       "  'index': 5,\n",
       "  'word': 'Kumar',\n",
       "  'start': 12,\n",
       "  'end': 17},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.994769,\n",
       "  'index': 12,\n",
       "  'word': 'T',\n",
       "  'start': 31,\n",
       "  'end': 32},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9623844,\n",
       "  'index': 13,\n",
       "  'word': '##UM',\n",
       "  'start': 32,\n",
       "  'end': 34},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.72965735,\n",
       "  'index': 22,\n",
       "  'word': 'AI',\n",
       "  'start': 73,\n",
       "  'end': 75}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(\" I am Pavan Kumar a stduent in TUM university, I like to learn about the AI and computer vision also\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "ner_1 = pipeline( \"ner\", model = \"FacebookAI/xlm-roberta-large-finetuned-conll03-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.9999907,\n",
       "  'index': 3,\n",
       "  'word': '▁Pa',\n",
       "  'start': 5,\n",
       "  'end': 7},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99998033,\n",
       "  'index': 4,\n",
       "  'word': 'van',\n",
       "  'start': 7,\n",
       "  'end': 10},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99999213,\n",
       "  'index': 5,\n",
       "  'word': '▁Kumar',\n",
       "  'start': 11,\n",
       "  'end': 16},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.99593395,\n",
       "  'index': 11,\n",
       "  'word': '▁',\n",
       "  'start': 30,\n",
       "  'end': 31},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.99858934,\n",
       "  'index': 12,\n",
       "  'word': 'TUM',\n",
       "  'start': 30,\n",
       "  'end': 33},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.69067717,\n",
       "  'index': 21,\n",
       "  'word': '▁AI',\n",
       "  'start': 72,\n",
       "  'end': 74}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_1('I am Pavan Kumar a stduent in TUM university, I like to learn about the AI and computer vision also')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
