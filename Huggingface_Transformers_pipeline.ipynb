{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3 align = center>Hugging face transformers for sentiment analysis.</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers pipeline makes it easy to access the models like classifier, tokenizer or any other available model in Hugging face to performe the diffrent tasks such as NLP, Computer vision etc.. [Tasks](https://huggingface.co/tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline for `named entity recognition` Here let's choose a model available in hugging face. [Reference](https://huggingface.co/learn/nlp-course/en/chapter1/3?fw=pt), [Choosen model](https://huggingface.co/models?pipeline_tag=token-classification&sort=trending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline('ner', model = \"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.99910825,\n",
       "  'index': 3,\n",
       "  'word': 'Pa',\n",
       "  'start': 6,\n",
       "  'end': 8},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.97229,\n",
       "  'index': 4,\n",
       "  'word': '##van',\n",
       "  'start': 8,\n",
       "  'end': 11},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9955035,\n",
       "  'index': 5,\n",
       "  'word': 'Kumar',\n",
       "  'start': 12,\n",
       "  'end': 17},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.994769,\n",
       "  'index': 12,\n",
       "  'word': 'T',\n",
       "  'start': 31,\n",
       "  'end': 32},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9623844,\n",
       "  'index': 13,\n",
       "  'word': '##UM',\n",
       "  'start': 32,\n",
       "  'end': 34},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.72965735,\n",
       "  'index': 22,\n",
       "  'word': 'AI',\n",
       "  'start': 73,\n",
       "  'end': 75}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(\" I am Pavan Kumar a stduent in TUM university, I like to learn about the AI and computer vision also\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "ner_1 = pipeline( \"ner\", model = \"FacebookAI/xlm-roberta-large-finetuned-conll03-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.9999907,\n",
       "  'index': 3,\n",
       "  'word': '▁Pa',\n",
       "  'start': 5,\n",
       "  'end': 7},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99998033,\n",
       "  'index': 4,\n",
       "  'word': 'van',\n",
       "  'start': 7,\n",
       "  'end': 10},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99999213,\n",
       "  'index': 5,\n",
       "  'word': '▁Kumar',\n",
       "  'start': 11,\n",
       "  'end': 16},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.99593395,\n",
       "  'index': 11,\n",
       "  'word': '▁',\n",
       "  'start': 30,\n",
       "  'end': 31},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.99858934,\n",
       "  'index': 12,\n",
       "  'word': 'TUM',\n",
       "  'start': 30,\n",
       "  'end': 33},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.69067717,\n",
       "  'index': 21,\n",
       "  'word': '▁AI',\n",
       "  'start': 72,\n",
       "  'end': 74}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_1('I am Pavan Kumar a stduent in TUM university, I like to learn about the AI and computer vision also')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero_Short_Classifier. (Classifying the text without model trained explicitly on new data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'one day I will see the world',\n",
       " 'labels': ['travel', 'dancing', 'cooking'],\n",
       " 'scores': [0.9938651323318481, 0.0032737720757722855, 0.002861043205484748]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "classifier(sequence_to_classify, candidate_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am Pavan Kumar love to learn about the technology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 2572, 6643, 6212, 9600, 2293, 2000, 4553, 2055, 1996, 2974, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'pa', '##van', 'kumar', 'love', 'to', 'learn', 'about', 'the', 'technology']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1045, 2572, 6643, 6212, 9600, 2293, 2000, 4553, 2055, 1996, 2974]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try for different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = \"xlnet-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_2 = AutoTokenizer.from_pretrained(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_1 = tokenizer_2(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁I', '▁am', '▁Pa', 'van', '▁Kumar', '▁love', '▁to', '▁learn', '▁about', '▁the', '▁technology']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer_2.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35, 569, 3069, 3207, 14851, 564, 22, 1184, 75, 18, 913]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer_2.convert_tokens_to_ids(tokens)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('van', 'to')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_2.decode(3207), tokenizer_2.decode(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Special tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging face and Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Pavan Kumar love to learn about the technology\n",
      "{'input_ids': [101, 1045, 2572, 6643, 6212, 9600, 2293, 2000, 4553, 2055, 1996, 2974, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_points = tokenizer(text, return_tensors = \"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13])\n"
     ]
    }
   ],
   "source": [
    "print(ids_points['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(ids_points['input_ids']).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"my_trained_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(model_directory)\n",
    "model.save_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = AutoModel.SequenceClassification.from_pretrained(model_directory)\n",
    "my_model = AutoModelForSequenceClassification.from_pretrained(model_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
